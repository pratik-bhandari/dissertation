---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
bib-humanities: true
documentclass: book
bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

<!-- # Citations, cross-references, and collaboration {#cites-and-refs}  -->
<!-- \chaptermark{Citations and cross-refs} -->

# General statistical approach {#chapter-stats}
\chaptermark{Statistics}

## Linear regression

<!-- Analysis of Variance (ANOVA) has been one of the very commonly used statistical tools in psychology, and psycholinguistics [...CITE...], specifically for continuous data.
It is still a common method for continuous data like response times.
However, as early as 19XX, ABC ...CITE... had pointed out one problem in using ANOVA in psycholingusitics.
He pointed out that considering all items to have similar effect on the participants is indeed a fallacy.
Although this helped understand that items are also as different as participants, it did not solve the problem inherent in ANOVA (CITE). -->
In this thesis, we use binomial mixed effects logistic regression models with crossed random effects [@Baayen2008].
These models are, simply put, extensions of logistic regression models.
A logistic regression models a dependent variable (or an *outcome*, or a *response* variable) as a function of one or more independent predictor variables (or *factors*, or *explanatory* variables).
That is, an outcome $y$ is modeled as a function of explanatory variables $x_1, x_2, x_3..., x_n$, and an error term $\varepsilon$.


\begin{align} \label{eq:linear_regression}
y =
\alpha + 
\beta_{1}\cdot{x_1} + 
\beta_{2}\cdot{x_2} + ... +
\beta_{n}\cdot{x_n} + \varepsilon 
\end{align}

The intercept $\alpha$, and the regression coefficients $\beta_1, \beta_2,$ and $\beta_3$ for each explanatory variable are estimated to achieve the model that best fits the data.
Analysis of Variance (ANOVA) is a special case of logistic regression [@Chatterjee2012; @Vasishth2022] that is one of the most common statistical tools in psychology and psycholinguistics.
These linear regressions as shown above (Equation \ref{eq:linear_regression}) and ANOVA however, are not well suited for categorical data like response to multiple choice questions or yes/no questions, confidence ratings, etc.
For example, in all the experiments in the current thesis, the response variables are response accuracy, given binary correct/incorrect responses.
Output of linear regression model ranges from $+\infty$ to $-\infty$ while accuracy (or probability) ranges from 0 to 1.
Additionally, simple regression models do not take into account the variability across individual participants and items.
These problems in language sciences have been pointed out since as early as 1960s [@Coleman1964; @Clark1973]. <!-- ...CITE language as a fixed effects fallacy... ], and later [ ... ]. -->
They are addressed to some extent by binomial logistic regression, and for our purpose by incorporating mixed effects model to binomial logistic regression [@Baayen2008].

Below we briefly introduce binomial logistic regression and mixed effects model.
Then we show a simple example of how binomial logistic mixed effects model is used in our data analyses.

## Binomial logistic regression

The response variable in the experiments in this thesis are binary.
Participants' written response to what they hear are coded as either correct or incorrect.
A binomial logistic regression model is best suited for such a categorical data [@Jaeger2008].
We use the term logistic regression model and binomial logistic regression model interchangeably henceforth.

As the name suggests, the output variable in a logistic regression model is in logit scale.
The model therefore predicts logits of an outcome variable.
Logits are $\log$ with base $e$, i.e. $\ln$.

*Probability* ranges from 0 to 1 only, while *odds* range from 0 to $+\infty$.
Fitting a linear regression model with probability, or odds would assume the range to be between 0 and 1, or between 0 and $+\infty$ respectively.
This restricts the range, and is an incorrect assumption for a linear model.
Therefore, in a binomial logistic regression model, log-odds are used which range from $-\infty$ to $+\infty$.

A simple binomial logistic regression model is shown in Equation \ref{eq:simple_logistic}:

\begin{align} \label{eq:simple_logistic}
\ln(\frac{p}{1-p}) =
\alpha + 
\beta_{1}\cdot{x_1} + 
\beta_{2}\cdot{x_2} + ... +
\beta_{n}\cdot{x_n} + \varepsilon
\end{align}

This is equivalent to,

\begin{align} \label{eq:logit-to-prob_long}
p &=
{\frac{exp(\alpha + 
\beta_{1}\cdot{x_1} + 
\beta_{2}\cdot{x_2} + ... +
\beta_{n}\cdot{x_n} + \varepsilon)}
{1 + exp (\alpha + 
\beta_{1}\cdot{x_1} + 
\beta_{2}\cdot{x_2} + ... +
\beta_{n}\cdot{x_n} + \varepsilon)}}\\ \label{eq:logit-to-prob_short}
&= {\frac{exp(\ln(\frac{p}{1-p}))}{1 + exp (\ln(\frac{p}{1-p}))}}
\end{align}

where,

\begin{align} \label{eq:logiteq}
\ln(\frac{p}{1-p}) =
{logit}(p)
\end{align}

<!-- \begin{align} \label{eq:logit-to-prob} -->
<!-- Pr(Y_i=1|X_i) = -->
<!-- {\frac{exp(\beta_0 + \beta_1X_i + \beta_2X_2 + ... + \beta_3X_{n})} -->
<!-- {1 + exp (\beta_0 + \beta_1X_i + \beta_2X_2 + ... + \beta_3X_{n})}} -->
<!-- \end{align} -->
Log-odds of correct response obtained from Equation \ref{eq:simple_logistic} can be transformed to probability of correct response. Equations \ref{eq:logit-to-prob_short}, and \ref{eq:logiteq} provide the relationship between probability, logit (or log-odds), and odds ($\frac{p}{1-p}$).

Some of the assumptions made for binomial logistic regression models are violated in our data.
One of them being non-independence of observations, i.e., all data points are independent from one another.
This assumption is violated in unbalanced design, and at times even for balanced design.
Same participant responds to multiple trials of same experimental condition within an experiment.
Although the design itself is balanced, after removal of outliers and/or trials which are not appropriate for comprehension measures, number of trials in analyses are unequal for each participant, item, and experimental condition.
This introduces a bias in the model [@Jaeger2008].

Another intrinsic property or feature of logistic regression is that it assumes a common mean for each predictor.
It has been shown that this is in fact not true: the effect of a predictor can vary depending on different random variables like participants, or items.
To account for these variances, mixed effects models are used.
In recent days, such statistical models are frequently used and advocated for by psycholingustis and statisticians [@Gries2015; @Meteyard2020]. 

## Mixed effects modeling

To overcome the limitations of logistic models, like violation of assumption of non-dependence of observations, and to account for the variability in the subject and/or item related parameter, mixed effects models are used.
Mixed effects models contain 1) both linear and logistic regressions, and 2) *fixed effects* and *random effects*, hence the name *mixed effects*.
Fixed effects term, e.g., levels of degradation assumes that all levels of degradation used in the experiment are independent from one another and they share a common residual variance.
The random effects term with only varying intercept, e.g., subject as intercept, assumes that if there are 100 subjects then the mean accuracy of those 100 subjects is only a subset of possible global accuracies drawn from a set of population mean.
When a slope, e.g., levels of predictability, is included to the random effects structure in addition to the varying intercept (e.g., subjects), then the model assumes that the effect of predictability on response accuracy varies across subjects.

## Binomial logistic mixed effects modeling {#binomial-logistic-mixed-effects-model}

A binomial logistic mixed effects model with varying intercepts and slopes for items and subjects is shown in Equation \ref{eq:mixed_effects} below.
<!-- \begin{align} -->
<!-- \ln (\frac{p}{1-p}) = \nonumber\\ -->
<!-- \alpha + u_{\alpha} + w_{\alpha} + \nonumber\\ -->
<!-- (\beta_{1} + u_{\beta_{1}} + w_{\beta_{1}})\cdot {x_1} + \nonumber\\ -->
<!-- (\beta_{2} + u_{\beta_{2}} + w_{\beta_{2}})\cdot {x_2} + ... + \nonumber\\ -->
<!-- (\beta_{n} + u_{\beta_{n}} + w_{\beta_{n}})\cdot {x_n} -->
<!-- \end{align} -->
\begin{align} \label{eq:mixed_effects}
\ln (\frac{p}{1-p}) = \alpha + u_{\alpha} + w_{\alpha} +
                      (\beta_{1} + u_{\beta_{1}} + w_{\beta_{1}})\cdot {x_1} + \nonumber\\
                      (\beta_{2} + u_{\beta_{2}} + w_{\beta_{2}})\cdot {x_2} + ... +
                      (\beta_{n} + u_{\beta_{n}} + w_{\beta_{n}})\cdot {x_n} 
\end{align}

where,

- $\alpha$ is the Intercept.
- Fixed effects: $\beta_{1}, \beta_{2}, ..., \beta_{n}$ are the coefficients (or effects) of $x_1, x_2, ...,x_n$.
- $\boldsymbol{u} = \langle u_{\alpha}, u_{\beta_1}, u_{\beta_2}, ..., u_{\beta_n} \rangle$ : Varying intercept and slopes for random effect term like, *subject*.
- $\boldsymbol{w} = \langle w_{\alpha}, w_{\beta_1}, w_{\beta_2}, ..., w_{\beta_n} \rangle$ : Varying intercept and slopes for random effect term like, *item*.


In the next chapters of this thesis, we perform statistical analyses of the effect of predictability, speech degradation and speech rate on response accuracy.
These variables are used in the fixed effects term.
Subjects and items are used as random intercepts with by-subject and by-item slopes.
The details of the models fitted to data from each experiment are given in Chapters 5, 6, and 7.

We therefore use binomial logistic mixed effects model as our main statistical analysis tool in all the experiments reported in this thesis.
We follow the recommendations of @Baayen2008, @Barr2013, and @Bates2015a.

## Running mixed effects models in R {#analysis-main}

Data preprocessing and analyses were performed in R-Studio (Version 3.6.1; R Core Team, 2019; Version 3.6.3; R Core Team 2020).
Accuracy was analyzed with Generalized Linear Mixed Models (GLMMs) with lmerTest [@Kuznetsova2017] and lme4 [@Bates2015] packages.
Binary responses (correct responses coded as 1 and incorrect responses coded as 0) for all participants were fit with a binomial logistic mixed effects model.
<!-- Noise condition (categorical; 4-, 6-, and 8-bands), target word predictability (categorical; HP, MP, LP), global noise context (categorical; predictable channel context and unpredictable channel context), and the interaction of noise condition and target word predictability were included in the fixed effects. -->

On the data from each experiment, we fitted models with maximal random effects structure that included random intercepts for each participant and item [@Barr2013].
By-participant and by-item slopes included in the model are discussed in the Analysis sections of Chapters 5, 6, and 7.
<!-- To find the optimal model for the data, non-significant higher-order interactions were excluded from the fixed-effects structure (and from the random-effects structure) in a stepwise manner. -->
Model selection was based on Akaike Information Criterion [@Grueber2011; @Richards2011] unless otherwise stated.
Random effects not supported by the data that explained zero variance according to singular value decomposition were excluded to prevent overparameterization [@Bates2015a].
This gave a more parsimonious model which was then extended separately with: i) item-related correlation parameters, ii) participant-related correlation parameter, and iii) both item- and participant-related correlation parameters.
The best fitting model among the parsimonious and extended models was then selected as the optimal model for our data.
<!-- ## Bayesian statistical methods -->
 
<!-- https://tex.stackexchange.com/questions/75764/how-do-you-make-numbered-equations-in-latex -->