---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
bib-humanities: true
documentclass: book
bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

<!-- # Citations, cross-references, and collaboration {#cites-and-refs}  -->
<!-- \chaptermark{Citations and cross-refs} -->

# General statistical approach {#chapter-stats}
\chaptermark{Statistics}

## Linear mixed effects models

As the name suggests, the linear mixed effects model (LME) is a linear regression model that consists of both fixed and random effects.
It allows modelling the underlying structure of the data, which includes
the standard fixed effects like the levels of speech degradation and the levels of target word predictability,
as well as random effects like items and participants.
These random effects are assumed to be random samples drawn from the general population.
In this thesis, the dependent variable (an *outcome* or a *response* variable) is binary (correct vs incorrect response).
So, we use binomial logistic mixed effects models with crossed random effects to model the data [@Baayen2008].

A linear mixed effects model can be written as:

  
\begin{align} \label{eq:mixed-effects1}
y = \alpha + u_{\alpha} + w_{\alpha} +
    (\beta_{1} + u_{\beta_{1}} + w_{\beta_{1}})\cdot {x_1} + \nonumber\\
    (\beta_{2} + u_{\beta_{2}} + w_{\beta_{2}})\cdot {x_2} + ... + \nonumber\\
    (\beta_{n} + u_{\beta_{n}} + w_{\beta_{n}})\cdot {x_n} 
\end{align}
  

where,

- $y$ is the dependent variable, like participant's response (correct vs. incorrect)
- $\alpha$ is the Intercept.
- Fixed effects: $\beta_{1}, \beta_{2}, ..., \beta_{n}$ are the coefficients (or effects) of $x_1, x_2, ...,x_n$.
- $\boldsymbol{u} = \langle u_{\alpha}, u_{\beta_1}, u_{\beta_2}, ..., u_{\beta_n} \rangle$ : Varying intercept and slopes for random effect term like, *subject*.
- $\boldsymbol{w} = \langle w_{\alpha}, w_{\beta_1}, w_{\beta_2}, ..., w_{\beta_n} \rangle$ : Varying intercept and slopes for random effect term like, *item*.

In contrast to linear regression models, mixed effects models allow to simultaneously account for the effects of two random variables, like item and participants.
The variance in the categorical dependent variable is also preserved, which would otherwise be eliminated by averaging in linear regression models.
<!--This is not possible in simple linear regression models like Analysis of Variance (ANOVA) [@Chatterjee2012; @Vasisth2022] which is a very common statistical tool in psychology and psycholinguistics.
Being a linear regression model, ANOVA is also, therefore, limited in its effectiveness when compared to mixed effects model as it cannot account for the variation in participants and items at the same time.-->
We discuss these issues and the motivation to use the mixed effects model in this thesis in more detail below in this chapter.
<!-- In fact, the problem of using ANOVA in language sciences have been pointed out since as early as the 1960s [@Clark1973; @Coleman1964].
Mixed effects models, specifically the binomial logistic mixed effects models, are best suited for the data presented in this thesis. -->
<!--In contrast to simple linear regression, mixed effects models do not require averaging accuracy over item and subject,
and preserve the variability of participants' response.
Participants' performance over the course of the experiment can also be kept track of without sacrificing the trial-by-trial variability.-->

### Linear regression and its limitations

In linear regression, a dependent variable (or an *outcome*) is modelled as a function of one or more independent predictor variables (*factors* or *explanatory* variables).
That is, an outcome $y$ is modelled as a function of explanatory variables $x_1, x_2, x_3..., x_n$, and an error term $\varepsilon$.

  
\begin{align} \label{eq:linear-regression}
y =
\alpha + 
\beta_{1}\cdot{x_1} + 
\beta_{2}\cdot{x_2} + ... +
\beta_{n}\cdot{x_n} + \varepsilon 
\end{align}
  

Analysis of Variance (ANOVA), also a form of linear regression [@Chatterjee2012; @Vasishth2022], compares the means and variances of two or more conditions.
As expressed in Equation \@ref(eq:linear-regression), regression models can only model fixed effects.
Although ANOVA can account for one random effect at a time, it still averages out the variance in the second random effect.
These problems of using ANOVA in language sciences have been pointed out as early as the 1960s [@Clark1973; @Coleman1964].
We elaborate on them in the context of the data of our experiments as follows.

<!-- ### Limitations of linear regression -->

#### Modeling two random effects simultaneously, and Variability in the data

As mentioned above, a simple linear regression model, including ANOVA, does not model the effect of two random effects<!-- --- participants and items --- --> simultaneously, which a mixed effects model does.
In the traditional ANOVA approach, researchers often run two separate regression models [@Lorch1990] by averaging raw data across participants, and items.
<!-- This further complicates the results. -->
Averaging eliminates the variability in the data.
Additionally, comparing the means of a categorical variable (correct vs incorrect responses) even when transformed into accuracy or proportion scale is hard to interpret sensibly compared to a continuous variable like reaction time [for discussion, see @Bolker2009; @Jaeger2008].
The statistical remedy for these problems in analyzing the data of our experiments is to apply mixed effects models.

#### Non-independence of observations

Some of the assumptions made for regression models are violated in our data.
One of them is the non-independence of observations, i.e., all data points are independent of one another.
This assumption is violated in an unbalanced design, and sometimes even for a balanced design.
The same participant responds to multiple trials of the same experimental condition within an experiment.
Although the design itself is balanced, after the removal of outliers and trials which are not appropriate for comprehension measures (for details, see Section \@ref(chapter-6-measurement)),
the number of trials in the analysis is unequal for each participant, item, and experimental condition.
This inequality or unbalance introduces a bias in the regression model [@Jaeger2008].
A mixed effects model is best suited for such unbalanced data.

#### Common mean for each predictor

An intrinsic property or feature of the linear regression model is that it assumes a common mean for each predictor.
It has been shown that this is, in fact, not true in the actual data:
the effect of a predictor can vary depending on random variables like participant or item.
Mixed effects models take into account such inter-participant and inter-item variability present in the data.
<!-- To account for these variances, mixed effects models are used. -->
For example, in mixed effects models,
the random effects term with only varying intercept, e.g., participant as an intercept, assumes that if there are 100 participants, then the mean accuracy of those 100 participants is only a subset of possible global accuracies drawn from a set of the population mean.
When a slope, e.g., levels of predictability, is included in the random effects structure in addition to the varying intercept (e.g., participants), then the model assumes that the effect of predictability on response accuracy varies across participants.
Such variance across participants (or across items) is present in the real data
and can be modelled in a mixed effects model but not in a linear regression model.
<!-- For such reasons, mixed models are used and advocated for by psycholingustis and statisticians [@Barr2013; @Gries2015; @Meteyard2020]. -->

#### Bounded output variable

Linear models assume an output variable to not be bounded within a narrow range and to be on a continuous scale.
In our data, the output variable (correct vs incorrect response) has bounded outcomes on $[0,1]$.
To fit a linear model, it can be transformed into a proportion scale.
Even though it is a continuous variable, the proportion scale (i.e., response accuracy) has a range (0,1).
Additionally, the transformation of discrete variables brings a host of problems that we have already discussed above (e.g., loss of variability by averaging raw data).
Binomial logistic mixed effects models, on the other hand, transform[^logit-link-footnote] the output variable into a *logit* scale, $\log$ with base $e$, i.e. $\ln$, with a range $(-\infty, +\infty)$.
<!-- where logit is equal to *log-odd* (see Equation \ref{eq:logiteq}). -->
Therefore, these mixed effects models do not violate the model assumptions regarding the range of the outcome variables.

Thus, Equation \@ref(eq:mixed-effects1) <!--Equation \ref{eq:mixed-effects2}--> can also be written as: 

  
\begin{align} \label{eq:mixed-effects2}
\ln (\frac{p}{1-p}) = \alpha + u_{\alpha} + w_{\alpha} +
                      (\beta_{1} + u_{\beta_{1}} + w_{\beta_{1}})\cdot {x_1} + \nonumber\\
                      (\beta_{2} + u_{\beta_{2}} + w_{\beta_{2}})\cdot {x_2} + \nonumber\\
                      ... + (\beta_{n} + u_{\beta_{n}} + w_{\beta_{n}})\cdot {x_n} 
\end{align}
  

This is equivalent to,

  
\begin{align} \label{eq:logit-to-prob}
p = {\frac{exp(\ln(\frac{p}{1-p}))}{1 + exp (\ln(\frac{p}{1-p}))}}
\end{align}
  
 
where,

  
\begin{align} \label{eq:logiteq}
\ln(\frac{p}{1-p}) =
{logit}(p)
\end{align}
  

Log-odds of correct response obtained from Equation \@ref(eq:mixed-effects2) can also be transformed into the probability of correct response.
Equations \@ref(eq:logit-to-prob) and \@ref(eq:logiteq) provide the relationship between probability, logits (or log-odds), and odds ($\frac{p}{1-p}$).  

We have presented the advantages of mixed effects models over linear (regression) models.
We use the binomial logistic mixed effects model as the statistical analysis tool in all experiments reported in this thesis.
Below we discuss how the model that best fits our data was selected.  

<!--In the next chapters of this thesis, we perform statistical analyses of the effect of predictability, speech degradation and speech rate on response accuracy.
These variables are used in the fixed effects term.
Participant and item are used as random intercepts with by-participant and by-item slopes.
The details of the models fitted to data from each experiment are given in Chapters 5, 6, and 7.
We therefore use binomial logistic mixed effects model as our main statistical analysis tool in all experiments reported in this thesis.
We follow the recommendations of @Baayen2008, @Barr2013, and @Bates2015a.-->

## Model selection and Running mixed effects models in R {#analysis-main}

The underlying structure of given data can be explained by different approximate statistical models.
We intend to select a model that best fits our data.
'Best fit' can be objectively measured by Akaike Information Criterion, Bayesian Information Criterion, and Likelihood Ratio Test, among others [@Schwarz1978; @Akaike1973].

In this thesis,
we first build a complex (or maximal) model by including all predictors (target word predictability, speech degradation level, speech rate),
their interactions, and co-variates (e.g., trial number) in the fixed effects [cf. @Bondell2010].
The model is fitted with a maximal random effects structure that includes random intercepts for each participant and item [@Barr2013].
By-participant and by-item slopes included in the model are discussed in the Analysis sections of Chapters 5, 6, and 7.  

Model selection was based on the backward-selection heuristics on the fixed effects [cf. @Matuschek2017].
To find the best fitting model for the data,
non-significant higher-order interactions were excluded from the fixed-effects structure in a stepwise manner.
Similarly, random effects not supported by the data that explained zero variance according to singular value decomposition were excluded to prevent overparameterization [@Bates2015a].
This gave a more parsimonious model, which was then extended separately with: i) item-related correlation parameters, ii) participant-related correlation parameters, and iii) both item- and participant-related correlation parameters.
Among the parsimonious model and extended models,
the model with the smallest AIC was selected as the best fitting model for our data [@Grueber2011; @Richards2011].  

Data preprocessing and analyses were performed in R-Studio (Version 3.6.1; R Core Team, 2019; Version 3.6.3; R Core Team 2020; Version 4.1.3; R Core Team, 2022).
Accuracy was analyzed with Generalized Linear Mixed Models (GLMMs) with lmerTest [@Kuznetsova2017] and lme4 [@Bates2015] packages.
Binary responses (correct responses coded as 1 and incorrect responses coded as 0) for all participants were fit with a binomial logistic mixed effects model.
Contrast coding of each factor and the model description are presented in the Analysis section of the chapters that follow.
<!-- Noise condition (categorical; 4-, 6-, and 8-bands), target word predictability (categorical; HP, MP, LP), global noise context (categorical; predictable channel context and unpredictable channel context), and the interaction of noise condition and target word predictability were included in the fixed effects. -->
<!-- ## Bayesian statistical methods -->
 <!-- \ref{eq:logit-to-prob} -->
<!-- https://tex.stackexchange.com/questions/75764/how-do-you-make-numbered-equations-in-latex -->

## Summary

In this chapter, we introduced the statistical tool used for data analysis in this thesis.
We discussed the limitations of traditional linear regression-based models like ANOVA
and outlined the motivations for using mixed effects models.
To capture the variability of our data without averaging out across participants or items,
and to account for the effect of two random effects --- participant and item --- simultaneously,
we fit mixed effects models to our data.  
Details of each dataset corresponding to each experiment are presented in Chapters 5, 6, and 7.  

<!--
\begin{align} \label{eq:logit-to-prob_long}
p &=
{\frac{exp(\alpha + u_{\alpha} + w_{\alpha} +
                      (\beta_{1} + u_{\beta_{1}} + w_{\beta_{1}})\cdot {x_1} + 
                      (\beta_{2} + u_{\beta_{2}} + w_{\beta_{2}})\cdot {x_2} + 
                      ... + (\beta_{n} + u_{\beta_{n}} + w_{\beta_{n}})\cdot {x_n} )}
{1 + exp (\alpha + u_{\alpha} + w_{\alpha} +
                      (\beta_{1} + u_{\beta_{1}} + w_{\beta_{1}})\cdot {x_1} + 
                      (\beta_{2} + u_{\beta_{2}} + w_{\beta_{2}})\cdot {x_2} + 
                      ... + (\beta_{n} + u_{\beta_{n}} + w_{\beta_{n}})\cdot {x_n})}}\\ \label{eq:logit-to-prob}
&= {\frac{exp(\ln(\frac{p}{1-p}))}{1 + exp (\ln(\frac{p}{1-p}))}}
\end{align}
-->

[^logit-link-footnote]: Such transformation is brought about by a generalized linear mixed effects model with a canonical logit link function [@Malik2020].  
