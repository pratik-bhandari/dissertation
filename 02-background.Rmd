---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

# Background {#chapter-background}

In the previous chapter, we outlined the theoretical background and the research goals of the studies in this dissertation.
We stated that the central theme of this thesis is to investigate the interaction between top-down predictive processes and bottom-up auditory processes in language comprehension.
Grounding on the noisy channel model of communication and predictive language processing,
the studies in this thesis manipulate the auditory processes $P(u_p|u_i,N)$, and prior information $P(u_i,m_i)$ in the form of semantic context available in a sentence.
In this chapter, we provide background on the noisy channel that was created and used to introduce variations in bottom-up processing in the studies presented in this thesis.
We also elaborate on the predictive language processing in the noisy channel, and the evidence of its limits and nature.
Understanding these fundamental concepts of top-down and bottom-up processes is essential for the chapters that follow;
these concepts are briefly reiterated in the following chapters wherever relevant.
Additionally, in this chapter we outline the gaps in previous research that this thesis fills in.

## Speech distortion and degradation {#distortion-degradation}

In an ideal situation, speech perception is a seamless process: a speaker produces an utterance, the speech signal transmits via a medium like air, and a listener perceives the signal as the speech waves enter her ears initiating a cascade of mechanical-neural processes of audition.
However, speech perception is hardly as smooth as it seems.
As shown earlier in Figure \@ref(fig:noisy-channel) and \@ref(fig:bayesian-network) in the noisy channel model of communication [@Gibson2013; @Gibson2019; @Levy2008; @Shannon1948],
due to the noise,
the speech signal uttered by the speaker gets disrupted and distorted
($u_i\rightarrow u_p\leftarrow N$).
<!--The major theories of speech *perception*, like Motor Theory, Direct Realist Theory, and General Auditory Framework focus on how listeners extract information from speech,
without considering the fact that there is noise in the signal.
For example, under the influential Motor Theory (MT), it is assumed that the components of speech perception are speech articulations (or articulatory events) instead of auditory or acoustic events.
The theory has been guided by the belief of a functional invariance in speech perception --- the objects of perception ought not vary.
According to the MT, humans' speech perception ability is not due to general auditory perception ability;
rather it is due to the specialized neurobiological makeup for language.
Other theories also suffer from similar shortcomings insofar describing speech perception in noise.-->
Distortion can occur at these three points or sources: encoding, transmission, and decoding [@Mattys2012].
Speech can be distorted while encoding the utterance $u_i$ due to the variability in speakers' production, like accented speech or slow and fast speech.
Distortion can arise while decoding the signal $u_p$ due to the listener-related factors, like hearing loss or auditory processing disorder.
It can also be a result of an external noise that appears during the transmission, like ambient noise, or poor medium of transmission (e.g., distortion in the telephone line).

Most of the existing theories of speech perception and language processing are inspired from the experiments that are conducted with clean speech,
the condition of "artificial normalcy" [@Mattys2012].
However, spoken language communication generally takes place outside the artificial normalcy:
in addition to the clean-sounding speaker's utterance, there are several sources of distortion we've outlined above.
These different sources of distortion make a listening condition adverse by affecting the time and frequency related properties/cues of the speech signal, i.e., temporal envelope cues and spectral details of speech respectively.
The temporal envelope cues are the slow variations in the amplitude of the speech signal over time [@Moon2014a; @Moon2014] while the spectral details are the frequency specific information about the speech.
The temporal envelope cues reflect the prosodic information of the speech, and are used in the lexical-semantic and syntactic processing [@Greenberg1996; @Schneider2001; @Sheldon2008b].
The spectral details provide information about the sound production reflecting the resonant properties of the vocal tract, frequency range of the speech signal, energy distribution across frequency bands, etc. [@Roberts2011; @Shannon2004; @Shannon1995].

In an experimental setup, a noisy channel can be created artificially by digital signal processing (see Section \@ref(speech-processing)) to investigate the response of the speech perception system to the distorted speech
and to study language comprehension in an adverse listening condition.
For example, expansion or compression of speech signal acts upon the temporal property of the speech and makes it slow or fast, and an optimal level of speech expansion/compression does not distort the spectral property of speech (see Section \@ref(compression-expansion)).
In addition to speech compression and expansion in Chapter \@ref(chapter-speech-rate),
throughout the studies in this thesis, we implement noise vocoding to manipulate the spectral property of speech and create a noisy channel of communication.

Noise vocoding removes the spectral details of the speech signal in a controlled manner only leaving intact its temporal and periodicity cues (see Section \@ref(noise-vocoding)).
This method of speech degradation was initially developed as a means to reduce the information in speech signal to be transmitted through the telephone line [@Dudley1939; @Vocoder1940].
Shannon and colleagues later modified and used this technique as an analogue to cochlear implant such that number of channels used in a cochlear implant are similar to the number of noise vocoding channels in terms of their speech output and intelligibility [@Shannon1995; @Loizou1999; @Shannon2004; cf. @Orena2021].
Therefore, in addition to being a method of speech distortion to parametrically vary and control the quality of speech signal in a graded manner,
noise vocoding is also a method of distortion that is used to understand the speech perception and language comprehension in cochlear implantees [e.g., @Patro2020; @Shannon2004; @Winn2016]

One of the main factors that determines the intelligibility of degraded speech is the number of noise vocoding channels.[^speech-degradation-footnote]
The higher the number of noise vocoding channels, the more is the frequency specific information available in the degraded speech,
consequently, higher is the intelligibility compared to the speech that is degraded with lesser number of noise vocoding channels.
For example, listeners are shown to rate 8 channels noise vocoded speech to be more intelligible and less effortful compared to 2 channels noise vocoded speech [e.g., @Obleser2011; @Sohoglu2012].
In our studies, we create noisy channel of different levels of degradation and intelligibility by noise vocoding the speech signal through 1, 4, 6 and 8 channels.<!-- which are based on prior studies on similar paradigm using similar methods and materials.-->
The details of the artificial distortions are described in Chapter \@ref(chapter-methods).

## Prediction and comprehension of degraded speech
<!-- We have stated above that the quality of speech signal determines the speech intelligibility. -->
<!--Listeners do not just rely on the quality of speech signal to understand degraded speech.
That is, it is not just the number of noise vocoding channels that determines how well a person understand degraded speech.-->
In addition to the quality of speech signal, listeners rely also on the context information and form top-down predictions to understand speech in adverse listening condition.
Below, we first review the role of predictions in language comprehension in general,
then we discuss the role of top-down predictive processes in comprehension of degraded speech in particular.

### Predictive language processing {#predictive-language-processing}

Research from various domains of cognitive (neuro)science, like emotion, vision, odor, and proprioception [the sensation of one's body position and movement, @Tuthill2018], has shown that perception and cognition can be described under the framework of predictive processing; they primarily operate by predicting upcoming events [@Stadler2012; @Clark2013; @Seth2013; @Marques2018; cf. @Bowers2012; @Jones2011; @Pierce1987].
Despite a long-standing skepticism and doubt about the usefulness of prediction in language [@Forster1981; @Jackendoff2002; @VanPetten2012],
human language comprehension too has been claimed to be predictive in nature from as early as mid-twentieth century [e.g., @Mccullough1958; @Miller1951; @Morton1964]<!-- @Bruce1958 for context in noise -->
which in recent days has received an overwhelming support from computational linguistics, psycholinguistics and cognitive neuroscience of language [e.g., @Delong2005; @Demberg2013; @Heyselaar2021; @Lupyan2015; @Pickering2018].
Empirical evidence from a number of studies suggests that readers and listeners predict upcoming words in a sentence when the words are predictable from the preceding context [for reviews, @Staub2015; @Kuperberg2016; @Nieuwland2019].
For instance, predictable words are skipped and read faster compared to the words that are less predictable from the context [@Ehrlich1981; @Frisson2005; @Staub2011].
Applying visual world paradigm, studies have demonstrated that individuals show anticipatory eye movements towards a picture of an object (e.g., *cake*) that is predictable from the preceding sentence context (e.g., *The boy will eat a...*) even before hearing the final target word [@Altmann1999; @Kamide2003; @Ankener2018].
Similar results have been observed in a virtual world set-up with naturalistic scenes [e.g., @Heyselaar2021].
The sentence-final word in a highly constraining sentence (e.g., *"She dribbles a ball."*) elicits a smaller N400 amplitude[^n400-footnote] than a less constraining sentence [e.g., *"She buys a ball."*, @Federmeier2007; @Kutas1984].
<!-- N400 is a negative-going EEG component that peaks around 400 ms post-stimulus, and it is considered as a neural marker of context-based semantic unexpectedness [@Kutas2011]. -->
Similarly, event-related words (e.g., *"luggage"*) elicited reduced N400 compared to event-unrelated words (e.g., *"vegetables"*) which were not predictable from the context [e.g., in an event of *"travel"*, @Metusalem2012].
<!-- ALSO ADD PAPER BY MAIRE STAUDE ON GRADED PREDICTION, i think it was an EEG and eye-tracking paper in 2020/21. -->
In sum, as the sentence context builds up, listeners make predictions about upcoming words in the sentence, and these in turn facilitate language comprehension.
That is, individuals use the context available to them to generate predictions which aids understanding written and spoken language.

#### But, what is prediction?
\noindent
The history of *prediction* in language science is rocky [@Husband2020].
People have been skeptical that language processing is predictive in nature.
Different people mean different things whey they use the word prediction.
As @Kuperberg2016 put it, *prediction* has become a loaded term;
it is used alogside other similar terms like *integration* [@Federmeier2007a], *anticipation*, *expectation*, [@VanPetten2012] *preparedness* [@Ferreira2018], etc.

In this thesis, we use the word prediction in the following minimal sense.
As a sentence unfolds, listeners encounter the context information in the sentence and they form its meaning representation, i.e.,
an internal representation of the context.
Before they hear the next word, i.e., before they encounter new bottom-up information,
they generate an expectation[^prediction-synonym] about the new word based on the meaning representation of the context.
They could form prediction about only the semantic feature of the next word or they could predict the exact word (meaning prediction vs word form prediction).

In reading studies and clean speech comprehension,
there are opposing views, whether comprehenders predictively pre-activate the upcoming linguistic unit solely based on the top-down information (i.e., predictive *pre-activation*)
or whether they wait for the bottom-up information to activate the representation (e.g., phonological, or semantic representation) of the new information and its neighbors,
then use the top-down information to select the best representation.
To clarify it further,
let's take the example sentence *(1)* presented in Chapter \@ref(chapter-introduction) on page \pageref{kite}:
*The day was breezy so the boy went outside to fly a*___.
Upon listening this context, the listeners can form a high degree of belief that the next word will be 'kite'.
Before even hearing it, listeners preactivate the representation of "kite" in their mental lexicon.
Alternatively, they could wait until they hear the auditory input "kite", which activates "kite" and its phonological and semantic neighbors in the mental lexicon,
then use the top-down information to select the the most likely word that completes the sentence.
Either way, top-down processes facilitate comprehension.

While listening in an adverse listening condition, it is unlikely that a listener follows the latter strategy of waiting for the bottom-up input to activate the representations and then select the most likely one based on the top-down information [@Kuperberg2016].
When speech is distorted, it is difficult to form the context representation in the first place.
Once a listener has formed a meaning representation of the context,
she cannot afford to again wait for the bottom-up input to activate phonological and/or semantic representations of upcoming words;
the uncertainty about the bottom-up information is persistent (see the phoneme restoration effect [@Warren1970], the McGurk effect [@McGurk1976], and the Ganong effect [@Ganong1980] in speech perception).
Thus, once the listener has formed a representation of the context,
she uses this top-down information to predictively preactivate what the upcoming word can be.
Such predictive preactivation can take different forms:
it can be<!--a single lexical (word-form) prediction or meaning prediction;-->probabilistic (graded) or deterministic (all-or-nothing) prediction.
<!-- It is important to understand if prediction is graded or all-or-nothing before we understand if it is word-form or word-meaning prediction. -->
<!-- In this thesis, we do not commit to word-form or meaning prediction 
(discussed in the context of experimental data in Chapter \@ref(chapter-speech-rate)).
However, we focus on the nature of prediction --- graded vs all-or-nothing --- in degraded speech comprehension.-->
This differences in the nature of prediction is discussed below.

### Facilitatory effect of predictability {#background-facilitatory-effect}

We have discussed above that individuals make predictions about not-yet-encountered linguistic units based on available context information as a sentence unfolds:
Top-down predictive and bottom-up perceptual processes interact dynamically in language comprehension.
When the bottom-up perceptual input is less reliable, for example, in an adverse listening condition, it has been shown that listeners rely more on top-down processes by narrowing down the predictions to smaller sets of semantic categories or words [@Corps2020; @Strauss2013].
Obleser and colleagues [@Obleser2007; @Obleser2010], for instance, used sentences of two levels of semantic predictability (high and low) and systematically degraded speech signal by passing it through various numbers of noise vocoding channels ranging from 1 to 32 in a series of behavioral and neuroimaging studies [see also @Hunter2018].
They found that semantic predictability facilitated language comprehension only at moderate levels of speech degradation.
That is, participants relied more on sentence context when the speech signal was degraded though *intelligible enough*, than when it was not degraded, or when it was highly degraded.
At such moderate levels of speech degradation, accuracy of word recognition was found to be higher for words in high predictability sentences than the words in low predictability sentences [@Obleser2010].
For the extremes, i.e., when the speech signal was highly degraded (making the speech almost completely unintelligible) or when it was the least degraded (rendering the speech intelligible),
the word recognition accuracy was similar across both levels of sentence predictability, meaning that predictability did not facilitate language comprehension.
@Sheldon2008b estimated that for both younger and older adults, the number of noise vocoding channels required to achieve 50\% accuracy varied as a function of sentence context.
To achieve the same level of accuracy as highly constraining sentences, a higher number of channels (i.e., more bottom-up information) was required in less constraining sentences.
They also concluded that when the speech is degraded, word recognition is facilitated by predictability and sentence context.
Taken together, these studies conclude that at moderate levels of degradation, participants rely more on the top-down predictions generated by a sentence context and less on the bottom-up perceptual processing of unclear, less reliable, and degraded speech signal [@Obleser2014].
<!--Reliance on prediction results in higher word recognition accuracy for target words high cloze probability than for target words with low cloze probability.
In the case of a heavily degraded speech signal, participants may not be able to understand the sentence context and, therefore, be unable to form predictions of the target word, or their cognitive resources may already be occupied by decoding the signal, leaving little room for making predictions.
Thus, there is no differential effect of levels of sentence predictability.
On the other extreme, when the speech is clear and intelligible, participants recognize the intelligible target word across all levels of sentence predictability.
Hence, no differential effect of levels of predictability of target word can be expected.-->
<!-- We consistently replicate [@Crandall2016; @Nosek2020] these findings which are presented in Chapters 5, 6, and 7. -->
<!--We first show that predictability facilitates comprehension of moderately degraded speech in Chapter 5, pointing out the importance of attention to context information.
Then we focus on the nature of such predictability effect.-->
However, these studies are agnostic about the nature of prediction, i.e., if it is probabilistic or deterministic.

#### Nature of prediction
\noindent
A debate in the literature of predictive language processing pertains to this question: Is prediction probabilistic, or is it an all-or-nothing phenomenon?
For instance, garden path phenomenon was explained as a parser's irreversible prediction about the sentence structure
which if fails (or, if it turns out to be incorrect), then the parser reanalyzes the sentence, and reformulates another prediction [e.g., @Ferreira1986; see also @Demberg2013; @Slattery2013].
In recent days, the support for the probabilistic nature of prediction comes, for example, from ERP studies that show an inverse and graded relationship between the magnitude of N400 effect evoked by a word and its predictability measured by cloze probability[^cloze-footnote] [e.g., @Delong2005; @Federmeier2007b], or *surprisal*[^surprisal-footnote] [@Frank2015].

These discussions come from reading studies and spoken language comprehension in clear speech.
Although a few frameworks of language processing speculate that language comprehension in adverse listening condition can be predictive [e.g., @Lowder2016; @Ryskin2018],
so far, only @Strauss2013 have investigated the nature of prediction in degraded speech comprehension.
They proposed an "expectancy searchlight model" which suggests that listeners form *narrowed expectations* from a restricted semantic space only when the sentence endings are highly predictable.
They rule out the graded nature of predictability.
<!--In this thesis, we take this theoretical account into consideration,
and examine the nature of prediction in degraded speech comprehension.-->
However, their approach to predictability was confounded by verb-noun association<!--rather than predictability *per se*.-->
(discussed in Chapter \@ref(chapter-graded-prediction)).
In contrast to their study, we systematically vary the predictability of target word,
and examine the graded vs. probabilistic nature of prediction in degraded speech comprehension.
We argue that the facilitatory effect of predictability is graded in nature;
it is not an all-or-nothing phenomenon focused solely on highly predictable sentence endings.
<!-- To fill this gap, we first establish in Chapter 5 that attention is a pre-requisite to predictive processing. -->
<!--In Chapter 6, we evaluate if the effect of predictability is focused only on highly predictable sentence endings.
From the results of two experiments, we argue that at prediction in degraded speech comprehension is not 'narrowed',
but it is probabilistic in nature.-->

### Limits of predictive language processing {#limits-of-pp}

It is important to note and acknowledge that the ubiquity and universality of predictive language processing has not gone unquestioned [@Huettig2016].
Apart from the debate on the nature of prediction, which we will come to later in this chapter, there is compelling evidence that questions the necessity of prediction in language comprehension.
For example, @Mishra2012 showed that literacy is a key factor that limits listeners' prediction about upcoming word.
In a visual world paradigm study, they found that individuals with lower literacy showed less anticipatory eye movements compared to the individuals with higher literacy.
They bolstered their finding in a neuroimaging study claiming that learning to read fundamentally changes the neural circuitry [@Hervais2019].
It is therefore plausible that such structural change in the brain is manifested in linguistic behavior.
Similarly, @Scholman2020 demonstrated that literacy is predictive of readers' sensitivity to discourse signals available in the context for the prediction of upcoming content.
Cognitive aging has also been shown to be a limiting factor in generating predictions.
Smaller N400 amplitude and latency in older adults compared to younger adults have been shown as an evidence of inability of older adults to engage in predictive language processing [@Federmeier2002].
Furthermore, among older adults, those with lower working memory scores are shown to be further disadvantaged when it comes to the use of context information [@Federmeier2010].
Another line of argument that critiques the predictive processing comes from the observations of @Huettig2019.<!--(see also Fernandez et al., 2020). [see also @Fernandez2020].-->
They analyzed participants' anticipatory eye movements in the visual world paradigm and showed that listeners predict the target word only in an *artificial* set-up of long preview time coupled with slow speech [cf. @Fernandez2020; @Heyselaar2021].
<!-- Alongside these critiques, there are also experiments demonstrating predictability effects that have been partly or fully replicated [e.g., @Ankener2019; @Nieuwland2018]. -->
<!-- Computational modeling of language processing also supports predictive nature of language processing [e.g., @Heilbron2020]. -->

In this thesis, we study additional top-down and bottom-up processes that can interact with, and potentially limit the facilitatory effect of predictability.
For example, current theories of predictive processing are poor in explaining the role of *attention* in semantic prediction [e.g., @Christiansen2015; @Ferreira2016; @Friston2020b; @Kuperberg2016; @Pickering2018].
<!--For example, good-enough processing [@Ferreira2016] postulates that readers (and by extension, listeners) do not perform deep processing of the bottom-up information.
They read/listen to text/speech and perform shallow processing that is enough to understand the gist of what is being said,
and readers/listeners can ignore anything that can seem irrelevant to understand the speech.-->
For example, in their prediction-by-production account, @Pickering2018 emphasize that listeners use their speech production mechanism in speech perception and comprehension, to predict what a speaker is going to say next.
Their framework attempts to paint a big picture of prediction --- using the motor system ---
but it does not consider how a listener's strategy of attending to only a part of speech stream in adverse listening condition influences linguistic predictions.
We argue that attention to context information is critical in forming semantic predictions,
especially in degraded speech comprehension [cf. @Kok2012].
By manipulating listeners' attention allocation to parts of speech stream and information content in the sentences, we show that attention to context information is a pre-requisite for the listeners to generate predictions.
<!-- We argue that listeners optimize their attentional resources to gather information from degraded speech. 
We manipulate listeners' attention to parts of speech stream,
and show that listeners' do not form top-down predictions when attentional resources are not allocated to context information in a sentence.-->
<!--Similarly, from @Pickering2018's *prediction-by-production* account of predictive language processing,
it follows that comprehenders make predictions (about word-form) at a slow presentation rate, but not at a fast presentation rate.-->
We also investigate the effect of bottom-up processes, like speech rate on top down processes (i.e., predictability effect in degraded speech comprehension).
The extant findings on the effects of speech rate on the facilitatory effect of predictability have been mixed both in clear and degraded speech comprehension [e.g., @Aydelott2004; @Goy2013; @Iwasaki2002; @Winn2021b].
We demonstrate that there is a scope for current theories of predictive language processing to incorporate
the instances of varying predictability effects at fast and slow speech rate,
as well as the effects of attention in degraded speech comprehension.

## Adaptation to degraded speech {#background-adaptation}

Listeners quickly adapt to novel speech with artificial acoustic distortions [@Dupoux1997]. 
With a repeated exposure to distorted speech, listeners' comprehension improves over time [for reviews, see @Samuel2009; @Guediche2014].
When the noise condition, like level of speech degradation is constant throughout the experiment, listeners adapt to it and the performance (e.g., word recognition) improves with as little as 20 minutes of exposure [@Rosen1999].
For example, @Davis2005[Experiment 1] presented listeners with 6 channels noise vocoded speech and found an increase in the proportion of correctly reported words over the course of experiment.
Similarly, @Erb2013 presented participants with 4 channels noise vocoded speech and reached a similar conclusion.
In these experiments, only one speech degradation level (6 or 4 channels noise vocoded speech) was presented in one block.
There was no uncertainty about the next-trial speech degradation <!--within a block-->from the participants' perspective.
<!-- i.e., the *global channel context* was certain or predictable. -->
Additionally, semantic feature (i.e., target word predictability) was not varied.<!--which we do in the experiments in this thesis.-->
When multiple types or levels of degraded speech signals are presented in a (pseudo-)randomized order within a block, then a listener is uncertain about the signal quality of any upcoming trial.
<!-- If such multiple levels of degradation are due to the presentation of multiple channels of noise vocoded speech, then the *global channel context* is unpredictable or uncertain. -->
This can influence perceptual adaptation such that
it might be totally absent with the change in the characteristics of auditory signal throughout an experiment [@Mattys2012].
In addition, trial-by-trial variability in the characteristics of distorted speech can impair word recognition [@Sommers1994; see also @Dahan2006].
<!--It can, thus, be speculated that if the noise vocoded speech varies from one trial to the next, then the adaptation to noise in this scenario might be different from the case in which spectral degradation is constant throughout the experiment.
Perceptual adaptation, however, is not limited to trial-by-trial variability of stimulus property.
Listeners can adapt to auditory signal at different time courses or time scales [@Atienza2002; see also @Whitmire2016].
In addition to the differences in intrinsic trial-by-trial variability (that is involved in short timescale trial-by-trial adaptation),
the global differences in the presentation of vocoded speech can result in the general adaptation at a longer timescale
which can differ between predictable and unpredictable channel contexts.-->
Only a limited number of studies has looked at how the (un)certainty about next-trial speech quality, and semantic feature influence adaptation.
For example, in a word-recognition task, @Vaden2013 presented words at +3dB SNR and +10dB SNR in a pseudo-random order;
the goal was to minimize the certainty about the noise level within the block.
They proposed that an adaptive control system might be involved to optimize the task performance when the listeners are uncertain about an upcoming trial [@Eckert2016; @Vaden2015; @Vaden2016].
However, we cannot make a firm conclusion about perceptual adaptation *per se* from their studies as they do not report the change in performance over the course of experiment.
Similarly, Obleser and colleagues [@Obleser2007; @Obleser2010; @Hartwigsen2015] also presented listeners with noise vocoded sentences (ranging from 2 to 32 channels noise vocoding) in a pseudo-randomized order but did not report the presence or absence of perceptual adaptation.
<!-- In the above-mentioned studies, the authors did not compare participants’ task performance in a blocked design against the presentation in a pseudo-randomized block of different degradation levels to make an inference about general adaptation to degraded speech at a longer timescale and adaptation at a shorter timescale on a trial-by-trial level. -->
On the one hand, repeated exposure is shown to lead to perceptual adaptation to degraded speech.
On the other hand, uncertainty about the speech quality is speculated to impair word recognition.
<!--Additionally, word-form predictions --- that the listeners are encouraged to make in the experiments reported in this thesis [^word-form-prediction] --- are shown to be inefficient in facilitating adaptation to degraded speech [@Corps2020].
Therefore, we investigate the role of certainty about next-trial speech degradation level and its semantic property (i.e., target word predictability) in perceptual adaptation.-->
<!--Although higher level lexical-semantic property of speech is assumed to drive perceptual adaptation,
the effect of the change in semantic property, namely predictability, on adaptation is unclear.
Chapter 5 addresses this question, and examines the role of (un)certainty about next-trial speech quality and its semantic feature in perceptual adaptation.-->
We argue that a trial-by-trial variation in higher level semantic feature of speech hinders listeners' perceptual system to retune itself to adapt to the lower level auditory features of the degraded speech [cf. @Nahum2008].
In contrast to prior studies, we show that listeners do not adapt to degraded speech despite a repeated exposure to the same degraded speech
as long as its semantic predictability is uncertain.

## Summary

In this chapter, we provided an overview of the concepts that will be repeated in the following chapters.
We introduced the concept of speech distortion and degradation.
Digital signal processing methods used in this process will be discussed in Chapter \@ref(chapter-methods) (Section \@ref(speech-processing)).
Importantly, we provided an overview of how predictive language processing aids language comprehension,
as well as its limitations.
We discussed perceptual adaptation to degraded speech, and the role of uncertainty about next-trial in adaptation.
At each step, we presented the motivation behind the studies in this thesis,
and the gaps in the literature these studies fill in.
In the next chapter, we will discuss the methods that are common in all the experiments (Chapters 5, 6, and 7) in developing materials and collecting data.

[^speech-degradation-footnote]: Throughout this thesis, speech distortion by noise vocoding is referred to as speech degradation, or spectral degradation of speech.
[^n400-footnote]: N400 is a negative-going EEG component that peaks around 400 ms post-stimulus, and it is considered as a neural marker of context-based semantic unexpectedness [@Kutas2011].
[^surprisal-footnote]: Surprisal is a measure of the change in probability mass (or simply put, the change in expectation) as predictions are proven wrong with an encounter of new words in a sentence, discourse, etc. [@Hale2001; @Smith2008].
[^cloze-footnote]: Cloze probability of a word is the proportion of participants who provide that word as the next word of a sentence, in an offline norming task, given the preceding words of the sentence [@Taylor1953; @Staub2015a] . It's value ranges from 0 to 1.
[^prediction-synonym]: Henceforth, we use the word expectation and prediction interchangeably.
<!-- [^word-form-prediction]: The possibility of the absence of word-form prediction in the experiments in this thesis is discussed in Chapter \@ref(chapter-speech-rate). -->